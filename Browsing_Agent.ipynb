{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvCTwXw4cStzZpIf7JNq33",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Syed-Raza-Ali/Browsing_AI_Agent/blob/main/Browsing_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install the required packages\n",
        "%%capture --no-stderr\n",
        "%pip install -U tavily-python langchain_core langchain_google_genai tavily-python langchain_community langgraph python-dotenv"
      ],
      "metadata": {
        "id": "_d3SOJXbmnJL"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-core langchain-google-genai langgraph python-dotenv"
      ],
      "metadata": {
        "id": "NVjOQOnMOXa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API KEYS VARIABLES\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n"
      ],
      "metadata": {
        "id": "8_3VvoTlzygI"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "IFgah5czmhKE",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f92059-a4e0-4471-a3d2-adb041c209f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello there! I am an browsing AI agent. Please give me a prompt, and I will bring you the answer by searching on Google\n",
            "Type 'exit' , 'quit' , 'q' , 'bye' for ending\n",
            "\n",
            "\n",
            "User : hey can you tell me today date\n",
            "Processing graph updates for: hey can you tell me today date\n",
            "Search Results:\n",
            "\n",
            "Result 1:\n",
            "Here is the 1 documentaion from web search you can get your answer from here:\n",
            "Documentation: Check out today's date and current time at your location and anywhere in the World. Today's date. Wednesday, December 25, 2024. Today is the day number 360 of the year, in the #52 week of the year 2024. Only 5 days left for 2025! Current time: 17:23. December 2024 calendar.\n",
            "URL: https://todays-date.net/\n",
            "\n",
            "Result 2:\n",
            "Here is the 2 documentaion from web search you can get your answer from here:\n",
            "Documentation: The current date in British Date Format (DD-MM-YYYY) with two-digit day, separator, two-digit month, separator, four-digit year is:\n",
            "The current date in American Date Format (MM-DD-YYYY) with two-digit month, separator, two-digit day, separator, four-digit year is:\n",
            "The current date in ISO 8601 Format (YYYY-MM-DD) with four-digit year, separator, two-digit month, separator, two-digit day is:\n",
            "The current date in SQL Date Format (YYYY-DD-MM) with four-digit year, separator, two-digit day, separator, two-digit month is:\n",
            "The current date in RFC 2822 Format with shortened day of week, numerical date, three-letter month abbreviation, year, time, and time zone is:\n",
            "The current date in Unix Epoch Format with number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT) is:\n",
            "Did You Learn What Is the Date Today?\n",
            " When Are You Going To Make The Purchases You've Been Procrastinating In The Last Days?\n",
            "Know that at the distance of a click you will find everything you need to plan your purchases today and do not procrastinate anymore.\n",
            " Know what is the exact current date in all regions of Planet Earth:\n",
            "What's The Date Today?\n",
            "Feeling the aftermath of last night's celebrations?\n",
            " Today's Date\n",
            "What Is Today's Date And Day?\n",
            "To correctly calculate today's date and time, you need to know your current time zone.\n",
            " For those of you who woke up hungover or lost in time, let's put judgments aside and let you know that today is Friday, on the 24th and the month is November of the year 2023.\n",
            "\n",
            "URL: https://calendarhours.com/todays-date/\n",
            "\n",
            "Result 3:\n",
            "Here is the 3 documentaion from web search you can get your answer from here:\n",
            "Documentation: Monday November 18, 2024 November 2024 Calendar Holidays 2024 Holidays 2025 Holidays November 2024 Holidays Todays Date Today's Moon Phases November Moon Phases Today's Date Today's Date is Monday November 18, 2024 Time zone: California/Mountain View Change Time zone  November 2024 Day Number of Year: 323 Month Number of Year:   11 Sun Today Moon Today Zodiac Signs and Birthday Symbols for Today's Date Nov 28 - Thurs  Thanksgiving Day 2024   Federal Holiday Dec 2 - Mon Cyber Monday 2024   Observance Dec 6 - Fri St Nicholas Day 2024    Christian Dec 7 - Sat Pearl Harbor Remebrance Day 2024    Observance Dec 17 - Tues   Wright Brothers Day 2024    Observance Site Map By using our site you consent to our Privacy Policy.\n",
            "URL: https://www.calendardate.com/todays.htm\n",
            "\n",
            "Chat History:\n",
            "User: hey can you tell me today date\n",
            "Assistant: [{'url': 'https://todays-date.net/', 'content': \"Check out today's date and current time at your location and anywhere in the World. Today's date. Wednesday, December 25, 2024. Today is the day number 360 of the year, in the #52 week of the year 2024. Only 5 days left for 2025! Current time: 17:23. December 2024 calendar.\"}, {'url': 'https://calendarhours.com/todays-date/', 'content': \"The current date in British Date Format (DD-MM-YYYY) with two-digit day, separator, two-digit month, separator, four-digit year is:\\nThe current date in American Date Format (MM-DD-YYYY) with two-digit month, separator, two-digit day, separator, four-digit year is:\\nThe current date in ISO 8601 Format (YYYY-MM-DD) with four-digit year, separator, two-digit month, separator, two-digit day is:\\nThe current date in SQL Date Format (YYYY-DD-MM) with four-digit year, separator, two-digit day, separator, two-digit month is:\\nThe current date in RFC 2822 Format with shortened day of week, numerical date, three-letter month abbreviation, year, time, and time zone is:\\nThe current date in Unix Epoch Format with number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT) is:\\nDid You Learn What Is the Date Today?\\n When Are You Going To Make The Purchases You've Been Procrastinating In The Last Days?\\nKnow that at the distance of a click you will find everything you need to plan your purchases today and do not procrastinate anymore.\\n Know what is the exact current date in all regions of Planet Earth:\\nWhat's The Date Today?\\nFeeling the aftermath of last night's celebrations?\\n Today's Date\\nWhat Is Today's Date And Day?\\nTo correctly calculate today's date and time, you need to know your current time zone.\\n For those of you who woke up hungover or lost in time, let's put judgments aside and let you know that today is Friday, on the 24th and the month is November of the year 2023.\\n\"}, {'url': 'https://www.calendardate.com/todays.htm', 'content': \"Monday November 18, 2024 November 2024 Calendar Holidays 2024 Holidays 2025 Holidays November 2024 Holidays Todays Date Today's Moon Phases November Moon Phases Today's Date Today's Date is Monday November 18, 2024 Time zone: California/Mountain View Change Time zone  November 2024 Day Number of Year: 323 Month Number of Year:   11 Sun Today Moon Today Zodiac Signs and Birthday Symbols for Today's Date Nov 28 - Thurs  Thanksgiving Day 2024   Federal Holiday Dec 2 - Mon Cyber Monday 2024   Observance Dec 6 - Fri St Nicholas Day 2024    Christian Dec 7 - Sat Pearl Harbor Remebrance Day 2024    Observance Dec 17 - Tues   Wright Brothers Day 2024    Observance Site Map By using our site you consent to our Privacy Policy.\"}]\n",
            "\n",
            "\n",
            "User : exit\n",
            "Bye! Have a great day!\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "\n",
        "# Initialize Tavily search tool\n",
        "tavily_search = TavilySearchResults(max_results = 3)\n",
        "\n",
        "# Maintain chat history\n",
        "chat_history = []\n",
        "\n",
        "def stream_graph_updates(user_input):\n",
        "    \"\"\"A placeholder function for processing graph updates.\"\"\"\n",
        "    print(f\"Processing graph updates for: {user_input}\")\n",
        "\n",
        "\n",
        "print(\"Hello there! I am an browsing AI agent. Please give me a prompt, and I will bring you the answer by searching on Google\")\n",
        "print(\"Type 'exit' , 'quit' , 'q' , 'bye' for ending\")\n",
        "\n",
        "\n",
        "def main():\n",
        "  \"\"\" Get realtime information using tavily search \"\"\"\n",
        "\n",
        "  while True:\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Prompt the user for input\n",
        "        print(\"\\n\")\n",
        "        user_input = input(\"User : \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\",\"bye\"]:\n",
        "            print(\"Bye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        # Handle empty input\n",
        "        if not user_input:\n",
        "            print(\"No prompt found. Please type something.\")\n",
        "            continue\n",
        "\n",
        "        # Add user input to chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Call the placeholder function (replace as needed)\n",
        "        try:\n",
        "            stream_graph_updates(user_input)\n",
        "        except NameError:\n",
        "            # Handle missing function gracefully\n",
        "            print(\"stream_graph_updates is not implemented. Skipping.\")\n",
        "\n",
        "        # Perform a Tavily search if the function is not implemented\n",
        "        search_docs = tavily_search.invoke(user_input)\n",
        "\n",
        "        # Add Tavily response to chat history\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": str(search_docs)})\n",
        "\n",
        "        # Display results from Tavily search\n",
        "        print(\"Search Results:\")\n",
        "        for i, doc in enumerate(search_docs, start=1):\n",
        "            print(f\"\\nResult {i}:\")\n",
        "            print(f\"Here is the {i} documentaion from web search you can get your answer from here:\")\n",
        "            print(f\"Documentation: {doc.get('content', 'N/A')}\")\n",
        "            print(f\"URL: {doc.get('url', 'N/A')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nChat History:\")\n",
        "    for message in chat_history:\n",
        "      role = message[\"role\"]\n",
        "      content = message[\"content\"]\n",
        "      print(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "def main_three():\n",
        "  \"\"\" Get realtime information using tavily search \"\"\"\n",
        "\n",
        "  while True:\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Prompt the user for input\n",
        "        print(\"\\n\")\n",
        "        user_input = input(\"User : \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\",\"bye\"]:\n",
        "            print(\"Bye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        # Handle empty input\n",
        "        if not user_input:\n",
        "            print(\"No prompt found. Please type something.\")\n",
        "            continue\n",
        "\n",
        "        # Add user input to chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Call the placeholder function (replace as needed)\n",
        "        try:\n",
        "            stream_graph_updates(user_input)\n",
        "        except NameError:\n",
        "            # Handle missing function gracefully\n",
        "            print(\"stream_graph_updates is not implemented. Skipping.\")\n",
        "\n",
        "        # Perform a Tavily search if the function is not implemented\n",
        "        search_docs = tavily_search.invoke()\n",
        "\n",
        "        # Add Tavily response to chat history\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": str(search_docs)})\n",
        "\n",
        "        # Display results from Tavily search\n",
        "        print(\"Search Results:\")\n",
        "        for i, doc in enumerate(search_docs, start=1):\n",
        "            print(f\"\\nResult {i}:\")\n",
        "            print(f\"Here is the {i} documentaion from web search you can get your answer from here:\")\n",
        "            print(f\"Documentation: {doc.get('content', 'N/A')}\")\n",
        "            print(f\"URL: {doc.get('url', 'N/A')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nChat History:\")\n",
        "    for message in chat_history:\n",
        "      role = message[\"role\"]\n",
        "      content = message[\"content\"]\n",
        "      print(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "tools = [main_three]\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", api_key = GOOGLE_API_KEY)\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "LE6fJSWHjAD-",
        "outputId": "d1e0f4e3-7fcf-4663-f303-b2122c4db5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected 'except' or 'finally' block (<ipython-input-90-85bf334f5b91>, line 58)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-90-85bf334f5b91>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    tools = [main_three]\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected 'except' or 'finally' block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import MessagesState\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# System message\n",
        "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing realtime searching using tavily \")\n",
        "\n",
        "# Node\n",
        "def assistant(state: MessagesState) -> MessagesState:\n",
        "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
      ],
      "metadata": {
        "id": "uqFJg60fjAGR"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import tools_condition\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from IPython.display import Image, display\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "\n",
        "# Graph\n",
        "builder: StateGraph = StateGraph(MessagesState)\n",
        "\n",
        "# Define nodes: these do the work\n",
        "builder.add_node(\"assistant\", assistant)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "# Define edges: these determine how the control flow moves\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
        "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
        "    tools_condition,\n",
        ")\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "react_graph: CompiledStateGraph = builder.compile()\n",
        "\n",
        "# Show\n",
        "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "P_YphZ9Ajr0-",
        "outputId": "61401503-ae96-4762-aa88-58d6da12dfa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content=\"hey can you tell me who is the prime minister of pakistan in 2024\")]\n",
        "messages = react_graph.invoke({\"messages\": messages})\n",
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "YB-a3IKnjr8V",
        "outputId": "03d0d5eb-0302-407a-8326-e7fa601c9bd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hey can you tell me who is the prime minister of pakistan in 2024\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "I cannot answer this question. The available tools lack the functionality to access and process real-time information, such as current political leaders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for m in messages['messages']:\n",
        "    m.pretty_print()"
      ],
      "metadata": {
        "id": "bOKtNtY3j-Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzlTw8fzj-SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1FazaIrJjAKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "def main_two():\n",
        "  \"\"\" Get realtime information using tavily search \"\"\"\n",
        "# Initialize Tavily search tool\n",
        "  tavily_search = TavilySearchResults(max_results = 3)\n",
        "\n",
        "# Maintain chat history\n",
        "  chat_history = []\n",
        "\n",
        "  print(\"Hello there! I am an browsing AI agent. Please give me a prompt, and I will bring you the answer by searching on Google\")\n",
        "  print(\"Type 'exit' , 'quit' , 'q' , 'bye' for ending\")\n",
        "\n",
        "  while True:\n",
        "\n",
        "    try:\n",
        "        # Prompt the user for input\n",
        "        print(\"\\n\")\n",
        "        user_input = input(\"User : \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\",\"bye\"]:\n",
        "            print(\"Bye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        # Handle empty input\n",
        "        if not user_input:\n",
        "            print(\"No prompt found. Please type something.\")\n",
        "            continue\n",
        "\n",
        "        # Add user input to chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Perform a Tavily search if the function is not implemented\n",
        "        search_docs = tavily_search.invoke(user_input)\n",
        "\n",
        "        # Add Tavily response to chat history\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": str(search_docs)})\n",
        "\n",
        "        # Display results from Tavily search\n",
        "        print(\"Search Results:\")\n",
        "        for i, doc in enumerate(search_docs, start=1):\n",
        "            print(f\"\\nResult {i}:\")\n",
        "            print(f\"Here is the {i} documentaion from web search you can get your answer from here:\")\n",
        "            print(f\"Documentation: {doc.get('content', 'N/A')}\")\n",
        "            print(f\"URL: {doc.get('url', 'N/A')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nChat History:\")\n",
        "    for message in chat_history:\n",
        "      role = message[\"role\"]\n",
        "      content = message[\"content\"]\n",
        "      print(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "\n",
        "\n",
        "# main_two()"
      ],
      "metadata": {
        "id": "49TNGMjdZk5g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm with tools\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key = GOOGLE_API_KEY\n",
        ")\n",
        "tool = [main_two, main]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tool)"
      ],
      "metadata": {
        "id": "fscgMBnuOlwf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import MessagesState, StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "\n",
        "# Disable LangSmith Tracing programmatically\n",
        "langchain_api_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Health-Agent\"\n",
        "\n",
        "# Initialize the Gemini LLM model with Google API Key\n",
        "\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "def main_two():\n",
        "  \"\"\" Get realtime information using tavily search \"\"\"\n",
        "# Initialize Tavily search tool\n",
        "  tavily_search = TavilySearchResults(max_results = 1)\n",
        "\n",
        "# Maintain chat history\n",
        "  chat_history = []\n",
        "\n",
        "  print(\"Hello there! I am an browsing AI agent. Please give me a prompt, and I will bring you the answer by searching on Google\")\n",
        "  print(\"Type 'exit' , 'quit' , 'q' , 'bye' for ending\")\n",
        "\n",
        "  while True:\n",
        "\n",
        "    try:\n",
        "        # Prompt the user for input\n",
        "        print(\"\\n\")\n",
        "        user_input = input(\"User : \").strip()\n",
        "\n",
        "        # Exit condition\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\",\"bye\"]:\n",
        "            print(\"Bye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "        # Handle empty input\n",
        "        if not user_input:\n",
        "            print(\"No prompt found. Please type something.\")\n",
        "            continue\n",
        "\n",
        "        # Add user input to chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Perform a Tavily search if the function is not implemented\n",
        "        search_docs = tavily_search.invoke(user_input)\n",
        "\n",
        "        # Add Tavily response to chat history\n",
        "        chat_history.append({\"role\": \"assistant\", \"content\": str(search_docs)})\n",
        "\n",
        "        # Display results from Tavily search\n",
        "        print(\"Search Results:\")\n",
        "        for i, doc in enumerate(search_docs, start=1):\n",
        "            print(f\"\\nResult {i}:\")\n",
        "            print(f\"Here is the {i} documentaion from web search you can get your answer from here:\")\n",
        "            print(f\"Documentation: {doc.get('content', 'N/A')}\")\n",
        "            print(f\"URL: {doc.get('url', 'N/A')}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nChat History:\")\n",
        "    for message in chat_history:\n",
        "      role = message[\"role\"]\n",
        "      content = message[\"content\"]\n",
        "      print(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "\n",
        "\n",
        "# main_two()\n",
        "\n",
        "\n",
        "# LangGraph nodes\n",
        "def model_conversation(state: MessagesState):\n",
        "    \"\"\"Core conversational logic.\"\"\"\n",
        "    system_message = \"you are a browsing ai assistant you have a tool name main_two which is for tavily search when ever user asked something you have to search using tool and then make enahce tavily responce then give anser to user\"\n",
        "    messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "tool_node = ToolNode(tools=tool)\n",
        "# workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Define the workflow\n",
        "workflow = StateGraph(MessagesState)\n",
        "# workflow.add_node(\"conversation\", model_conversation)\n",
        "# workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "\n",
        "workflow.add_node(\"conversation\", model_conversation)\n",
        "workflow.add_node(\"tavily\", main_two)\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", main_two)\n",
        "workflow.add_edge(\"tavily\", END)\n",
        "# Define conditional edges for the chatbot node based on tool usage\n",
        "# workflow.add_conditional_edges(\n",
        "#     \"conversation\",\n",
        "#     tools_condition\n",
        "# )\n",
        "\n",
        "# # Any time a tool is called, we return to the chatbot to decide the next step\n",
        "# workflow.add_edge(\"tools\", \"conversation\")\n",
        "\n",
        "# # Define the starting point of the graph as the chatbot node\n",
        "# workflow.add_edge(START, \"conversation\")\n",
        "\n",
        "# Compile the graph with memory\n",
        "# memory = MemorySaver()\n",
        "graph = workflow.compile()\n",
        "\n",
        "\n",
        "# Chatbot runtime logic\n",
        "def agent_calling():\n",
        "    \"\"\" Run the agent.\"\"\"\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in [\"exit\", \"bye\", \"quit\", \"q\" ]:\n",
        "            print(\"Goodbye! Thank you for your trust, I hope this information is helpful for you\")\n",
        "            break\n",
        "\n",
        "\n",
        "        input_message = HumanMessage(content=user_input)\n",
        "        output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "\n",
        "        # try:\n",
        "        #     output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error: {str(e)}\")\n",
        "        #     continue\n",
        "\n",
        "        bot_response = output[\"messages\"][-1].content\n",
        "        print(f\"Chatbot: {bot_response}\")\n",
        "\n",
        "        state = graph.get_state(config)\n",
        "\n",
        "\n",
        "\n",
        "# Run the chatbot\n",
        "agent_calling()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "XShuQsnpOTgX",
        "outputId": "ac440cad-d078-4076-c629-48e0d37c01dd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: who is the prime minister of US in 2020\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "main_two() takes 0 positional arguments but 1 was given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-57ca995a8550>\u001b[0m in \u001b[0;36m<cell line: 153>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m# Run the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m \u001b[0magent_calling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-57ca995a8550>\u001b[0m in \u001b[0;36magent_calling\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0minput_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_message\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1936\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   1937\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1657\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/graph/graph.py\u001b[0m in \u001b[0;36m_route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: main_two() takes 0 positional arguments but 1 was given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import MessagesState, StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "\n",
        "\n",
        "# Disable LangSmith Tracing programmatically\n",
        "langchain_api_key = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Health-Agent\"\n",
        "\n",
        "# Initialize the Gemini LLM model with Google API Key\n",
        "\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "def main_two(state=None):\n",
        "    \"\"\"Get real-time information using Tavily search.\"\"\"\n",
        "    # Initialize Tavily search tool\n",
        "    tavily_search = TavilySearchResults(max_results=1)\n",
        "\n",
        "    # # Maintain chat history\n",
        "    # chat_history = []\n",
        "\n",
        "    # print(\"Hello there! I am a browsing AI agent. Please give me a prompt, and I will bring you the answer by searching on Google.\")\n",
        "    # print(\"Type 'exit', 'quit', 'q', or 'bye' to end.\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Prompt the user for input\n",
        "            print(\"\\n\")\n",
        "            user_input = input(\"User : \").strip()\n",
        "\n",
        "            # Exit condition\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\", \"bye\"]:\n",
        "                print(\"Bye! Have a great day!\")\n",
        "                break\n",
        "\n",
        "            # Handle empty input\n",
        "            if not user_input:\n",
        "                print(\"No prompt found. Please type something.\")\n",
        "                continue\n",
        "\n",
        "            # Add user input to chat history\n",
        "            chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "            # Perform a Tavily search\n",
        "            search_docs = tavily_search.invoke(user_input)\n",
        "\n",
        "            # Add Tavily response to chat history\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": str(search_docs)})\n",
        "\n",
        "            # Display results from Tavily search\n",
        "            print(\"Search Results:\")\n",
        "            for i, doc in enumerate(search_docs, start=1):\n",
        "                print(f\"\\nResult {i}:\")\n",
        "                print(f\"Here is the {i} documentation from the web search you can get your answer from here:\")\n",
        "                print(f\"Documentation: {doc.get('content', 'N/A')}\")\n",
        "                print(f\"URL: {doc.get('url', 'N/A')}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "\n",
        "        # print(\"\\nChat History:\")\n",
        "        # for message in chat_history:\n",
        "        #     role = message[\"role\"]\n",
        "        #     content = message[\"content\"]\n",
        "        #     print(f\"{role.capitalize()}: {content}\")\n",
        "\n",
        "\n",
        "\n",
        "# LangGraph nodes\n",
        "def model_conversation(state: MessagesState):\n",
        "    \"\"\"Core conversational logic.\"\"\"\n",
        "    system_message = \"Whenever a user provides a prompt, respond to them by conducting real-time searching.\"\n",
        "    messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": response}\n",
        "tool_node = ToolNode(tools=tool)\n",
        "# workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Define the workflow\n",
        "workflow = StateGraph(MessagesState)\n",
        "# workflow.add_node(\"conversation\", model_conversation)\n",
        "workflow.add_node(\"tools\", tool_node)\n",
        "\n",
        "\n",
        "workflow.add_node(\"conversation\", model_conversation)\n",
        "# workflow.add_node(\"tavily\", main_two)\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "# workflow.add_conditional_edges(\"conversation\", main_two)\n",
        "# workflow.add_edge(\"tavily\", END)\n",
        "# Define conditional edges for the chatbot node based on tool usage\n",
        "workflow.add_conditional_edges(\n",
        "    \"conversation\",\n",
        "    main_two\n",
        ")\n",
        "\n",
        "# # Any time a tool is called, we return to the chatbot to decide the next step\n",
        "workflow.add_edge(\"tools\", \"conversation\")\n",
        "\n",
        "# # Define the starting point of the graph as the chatbot node\n",
        "# workflow.add_edge(START, \"conversation\")\n",
        "\n",
        "# Compile the graph with memory\n",
        "# memory = MemorySaver()\n",
        "graph = workflow.compile()\n",
        "\n",
        "\n",
        "# Chatbot runtime logic\n",
        "def agent_calling():\n",
        "    \"\"\" Run the agent.\"\"\"\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in [\"exit\", \"bye\", \"quit\", \"q\" ]:\n",
        "            print(\"Goodbye! Thank you for your trust, I hope this information is helpful for you\")\n",
        "            break\n",
        "\n",
        "\n",
        "        input_message = HumanMessage(content=user_input)\n",
        "        output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "\n",
        "        # try:\n",
        "        #     output = graph.invoke({\"messages\": [input_message]}, config)\n",
        "        # except Exception as e:\n",
        "        #     print(f\"Error: {str(e)}\")\n",
        "        #     continue\n",
        "\n",
        "        bot_response = output[\"messages\"][-1].content\n",
        "        print(f\"Chatbot: {bot_response}\")\n",
        "\n",
        "        state = graph.get_state(config)\n",
        "\n",
        "\n",
        "\n",
        "# Run the chatbot\n",
        "agent_calling()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "_Je-E9K7M08Q",
        "outputId": "62e2f7a5-ba0c-4bf6-d459-be0cae519e33"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: who is the  prime minister of US in 2024\n",
            "\n",
            "\n",
            "User : who is the prime minister of us in 2024\n",
            "Search Results:\n",
            "\n",
            "Result 1:\n",
            "Here is the 1 documentation from the web search you can get your answer from here:\n",
            "Documentation: Contact us; Contribute Help; Learn to edit; Community portal; Recent changes; Upload file; Search. Search. ... Prime Minister of Libya (Government of National Unity) Provisional government: 15 March 2021 ... 27 May 2024 211 days Judith Tuluka: Prime Minister of the Democratic Republic of the Congo: Semi-presidential republic 12 June 2024\n",
            "URL: https://en.wikipedia.org/wiki/List_of_current_prime_ministers_by_date_of_assumption_of_office\n",
            "\n",
            "\n",
            "User : exit\n",
            "Bye! Have a great day!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Branch did not return a valid destination",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-a41f9333703e>\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# Run the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0magent_calling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-a41f9333703e>\u001b[0m in \u001b[0;36magent_calling\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0minput_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput_message\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1936\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   1937\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1657\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/graph/graph.py\u001b[0m in \u001b[0;36m_route\u001b[0;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     async def _aroute(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/graph/graph.py\u001b[0m in \u001b[0;36m_finish\u001b[0;34m(self, writer, input, result, config)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mdestinations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSTART\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Branch did not return a valid destination\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEND\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdestinations\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot send a packet to the END node\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Branch did not return a valid destination"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.graph import MessagesState, StateGraph, START, END\n",
        "from langgraph.graph.state import CompiledStateGraph\n",
        "# from langchain.llms import LLMNode\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "def main_two(state=None):\n",
        "  \"\"\"\n",
        "  Performs real-time information retrieval using Tavily search\n",
        "  and feeds the results back to the LLM for further processing.\n",
        "  \"\"\"\n",
        "  # Initialize Tavily search tool\n",
        "  tavily_search = TavilySearchResults(max_results=1)\n",
        "\n",
        "  # Prompt the user for input\n",
        "  user_input = input(\"User: \").strip()\n",
        "\n",
        "  # Exit condition\n",
        "  if user_input.lower() in [\"quit\", \"exit\", \"q\", \"bye\"]:\n",
        "    return {\"message\": \"Bye! Have a great day!\"}\n",
        "\n",
        "  # Handle empty input\n",
        "  if not user_input:\n",
        "    return {\"message\": \"No prompt found. Please type something.\"}\n",
        "\n",
        "  # Perform a Tavily search\n",
        "  search_docs = tavily_search.invoke(user_input)\n",
        "\n",
        "  # Prepare data for LLM\n",
        "  data = {\n",
        "      \"prompt\": user_input,\n",
        "      \"tavily_results\": search_docs\n",
        "  }\n",
        "\n",
        "  # Forward data to LLM for processing (replace with your LLM integration)\n",
        "  llm_response = llm_with_tools.invoke(data)\n",
        "\n",
        "  return {\"message\": llm_response}\n",
        "\n",
        "# LangChain nodes\n",
        "def conversation(state: MessagesState):\n",
        "  \"\"\"\n",
        "  Core conversational logic.\n",
        "  - Forwards user prompt to Tavily for real-time search.\n",
        "  - Sends search results along with the prompt to the LLM for processing.\n",
        "  - Returns the LLM generated response.\n",
        "  \"\"\"\n",
        "  user_input = state[\"message\"]\n",
        "  response = main_two(user_input)\n",
        "  return {\"messages\": [SystemMessage(content=response[\"message\"])]}\n",
        "\n",
        "# Define the workflow\n",
        "workflow = StateGraph(MessagesState)\n",
        "workflow.add_node(\"conversation\", conversation)\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_edge(\"conversation\", END)\n",
        "\n",
        "# Compile the graph\n",
        "graph = workflow.compile()\n",
        "\n",
        "# Chatbot runtime logic\n",
        "def agent_calling():\n",
        "  \"\"\"\n",
        "  Runs the conversational agent.\n",
        "  \"\"\"\n",
        "  while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    if user_input.lower() in [\"exit\", \"bye\", \"quit\", \"q\"]:\n",
        "      print(\"Goodbye! Thank you for your trust, I hope this information is helpful for you\")\n",
        "      break\n",
        "\n",
        "    output = graph.invoke({\"message\": user_input})\n",
        "    bot_response = output[\"messages\"][0].content\n",
        "    print(f\"Chatbot: {bot_response}\")\n",
        "\n",
        "# Replace with your LLM integration (assuming llm_with_tools is the LLM node)\n",
        "# llm_with_tools = llm.bind_tools(tool_name=main_two)  # Replace with your LLM tool name\n",
        "\n",
        "# Run the chatbot\n",
        "agent_calling()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "BJnVTSkJYnaq",
        "outputId": "842601a7-c081-43f3-e621-c72506ce58e9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: who is the prime minister of US in 2024\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Must write to at least one of ['messages']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-deb1c1a7f88c>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Run the chatbot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0magent_calling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-deb1c1a7f88c>\u001b[0m in \u001b[0;36magent_calling\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"message\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mbot_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Chatbot: {bot_response}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1935\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1936\u001b[0;31m         for chunk in self.stream(\n\u001b[0m\u001b[1;32m   1937\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1657\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 )\n\u001b[1;32m    407\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         ]\n\u001b[0;32m---> 96\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langgraph/pregel/write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequire_at_least_one_of\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mchan\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequire_at_least_one_of\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 raise InvalidUpdateError(\n\u001b[0m\u001b[1;32m    158\u001b[0m                     \u001b[0;34mf\"Must write to at least one of {require_at_least_one_of}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 )\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Must write to at least one of ['messages']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "GqqSLhhyRbkg",
        "outputId": "211e53f4-ba74-4cf3-d4c0-a3cc57d7fbbf"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}